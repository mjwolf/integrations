# Service Info

## Common use cases

The Squid integration for Elastic allows administrators to monitor and analyze web traffic flowing through their caching proxy servers. By ingesting Squid access logs, organizations can gain deep visibility into web usage patterns, security threats, and proxy performance.
- **Security Monitoring and Auditing:** Track all outbound HTTP/HTTPS requests to identify connections to malicious domains, unauthorized file downloads, or potential data exfiltration attempts by internal users.
- **Bandwidth and Performance Optimization:** Analyze cache hit/miss ratios and response times to tune caching policies, reduce redundant external traffic, and improve the browsing experience for end-users.
- **Compliance and Policy Enforcement:** Maintain detailed records of web activity to meet regulatory requirements (such as HIPAA or PCI-DSS) and verify that users are adhering to corporate acceptable use policies.
- **Infrastructure Troubleshooting:** Quickly identify backend server failures, DNS resolution issues, or client-side connectivity problems by analyzing Squid result codes and hierarchy descriptors.

## Data types collected

This integration can collect the following types of data:
- **Access Logs:** Detailed records of every request processed by the Squid proxy, including client IP addresses, requested URLs, HTTP methods, and response status codes.
- **Syslog Data:** Standardized system messages generated by the Squid service, forwarded via the local syslog daemon or directly to a remote collector.
- **Traffic Metrics:** Information regarding data volume (bytes sent/received), request duration, and cache performance results (HIT, MISS, REFRESH_HIT).
- **Data Formats:** The integration supports standard Squid log formats, Common Log Format (CLF), and custom formats forwarded via Syslog, UDP, or TCP.
- **Log Locations:** Logs are typically retrieved from `/var/log/squid/access.log` or received via network protocols on ports such as 514 (Syslog) or custom UDP/TCP ports.

## Compatibility

The **Squid** integration is compatible with most modern versions of Squid (3.x, 4.x, 5.x, and 6.x) that support the `access_log` directive with network-based targets. It has been primarily tested with standard Linux-based distributions where Squid is deployed as a system service.

## Scaling and Performance

- **Transport/Collection Considerations:** The Squid integration primarily uses the Logfile collection method. Because Squid can generate a high volume of log entries (one per request), it is critical that the Elastic Agent has sufficient disk I/O to read the `access.log` file without contention. Using SSDs for the log partition is recommended for high-traffic proxies. Additionally, ensure that log rotation (e.g., via `logrotate`) is configured to prevent individual log files from becoming excessively large, which can impact initial read times during agent restarts.
- **Data Volume Management:** To manage the volume of data sent to the Elastic Stack, users should leverage Squid's native filtering capabilities. Use the `log_access` directive in `squid.conf` to exclude specific types of traffic that do not require monitoring, such as internal health checks or high-frequency automated API polling. Reducing the logging of "TCP_MEM_HIT" events for certain static assets can also significantly decrease index volume without losing critical security visibility.
- **Elastic Agent Scaling:** For large-scale environments with multiple Squid nodes, it is recommended to deploy an Elastic Agent on each proxy server (distributed model) rather than using a centralized syslog collector. A single Elastic Agent can typically process several thousand Squid log lines per second; however, if the proxy handles more than 5,000 requests per second, ensure the Agent is allocated at least 2 CPU cores and 4GB of RAM to handle the regex-based parsing load within the ingest pipeline.

# Set Up Instructions

## Vendor prerequisites

- **Administrative Access:** You must have `root` or `sudo` level access to the Linux host where Squid is installed to modify configuration files and read log directories.
- **Logging Enabled:** The Squid service must be configured to write access logs to a local file. This is usually enabled by default in `squid.conf`.
- **Permissions:** The user account running the Elastic Agent must have read permissions for the `/var/log/squid/` directory and all files within it.
- **Clock Synchronization:** Ensure the Squid host's clock is synchronized via NTP, as timestamp mismatches can lead to issues with data visualization and time-series analysis in Kibana.
- **Knowledge of Config Path:** You must know the exact path of your Squid configuration file, typically located at `/etc/squid/squid.conf`.

## Elastic prerequisites

- **Elastic Agent Installed:** The Elastic Agent must be installed on a host that is reachable by the Squid server and enrolled in Fleet.
- **Integration Policy:** A policy must be created in Kibana that includes the Squid integration with the correct input type (TCP, UDP, or Syslog).
- **Network Path:** The host running the Elastic Agent must have the configured listening port open to receive incoming traffic from the Squid proxy.

## Vendor set up steps

To configure Squid to send logs to the Elastic Agent, modify the `squid.conf` file using one of the following methods.

### For UDP/TCP Collection:
1. Open the Squid configuration file, typically located at `/etc/squid/squid.conf`, using a text editor like `vi` or `nano`.
2. Define a custom log format (optional) or use the default. To define a format named `elastic_format`, add:
   ```ini
   logformat elastic_format %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
   ```
3. Configure the destination. Replace `<elastic_agent_ip>` and `<port>` with your actual Elastic Agent details.
   - **For UDP:**
     ```ini
     access_log udp://<elastic_agent_ip>:<port> elastic_format
     ```
   - **For TCP:**
     ```ini
     access_log tcp://<elastic_agent_ip>:<port> elastic_format
     ```
4. Comment out any existing `access_log` directives that point to local files if you wish to stop local logging:
   ```ini
   # access_log daemon:/var/log/squid/access.log squid
   ```
5. Save the file and exit the editor.
6. Test the Squid configuration for syntax errors:
   ```bash
   sudo squid -k parse
   ```
7. Restart the Squid service to apply changes:
   ```bash
   sudo systemctl restart squid
   ```

### For Syslog Collection:
1. Open your `squid.conf` file.
2. Add the following line to forward logs to the local syslog facility:
   ```ini
   access_log syslog:local1.notice squid
   ```
3. Configure your local syslog daemon (e.g., rsyslog or syslog-ng) to forward these logs to the Elastic Agent IP and port.
4. If the Elastic Agent is listening directly for Syslog, ensure the facility and priority match your integration settings.
5. Save the configuration and restart Squid:
   ```bash
   sudo service squid restart
   ```

## Kibana set up steps

### For Logfile Collection:

1.  Log in to Kibana and navigate to **Management > Integrations**.
2.  In the search bar, type **Squid** and select the Squid integration tile.
3.  Click on **Add Squid**.
4.  Select the **Log file** input type (this is the default and most common method).
5.  Configure the **Log file path**. By default, this is set to `/var/log/squid/access.log*`. If your logs are in a different directory, update this field accordingly.
6.  In the **Advanced options**, you can optionally add tags or custom processors if you need to filter data before it reaches Elasticsearch.
7.  Under **Where to add this integration**, select an existing **Agent Policy** that is applied to your Squid host, or create a new policy.
8.  Click **Save and continue**, then select **Add agent to your hosts** if you haven't already installed the agent.
9.  Click **Save and deploy** to push the configuration to the Elastic Agent.

# Validation Steps

After configuration is complete, follow these steps to verify data is flowing correctly from Squid to the Elastic Stack.

### 1. Trigger Data Flow on Squid:
- **Generate Web Traffic:** From a client machine configured to use the Squid proxy, browse to several public websites (e.g., `http://www.elastic.co`). This will generate standard `TCP_MISS` or `TCP_HIT` entries in the access log.
- **Trigger a 404 Error:** Attempt to access a non-existent page on a web server through the proxy (e.g., `http://example.com/nonexistentpage`) to generate error status codes in the logs.
- **Reload Squid Service:** Run `sudo systemctl reload squid` to generate system-level cache logs indicating the service status and configuration reload.

### 2. Check Data in Kibana:
1.  Navigate to **Analytics > Discover**.
2.  Select the `logs-*` data view or create a specific view for Squid logs.
3.  Enter the following KQL filter: `data_stream.dataset : "squid.log"`
4.  Verify logs appear in the results. Expand a log entry and confirm these fields are populated:
    - `event.dataset` (should be `squid.log`)
    - `source.ip` (the IP address of the client making the request)
    - `url.original` (the full URL requested by the client)
    - `http.response.status_code` (the HTTP status returned to the client)
    - `squid.access.result_code` (Squid-specific code like `TCP_MISS` or `TCP_REFUSED_HIT`)
    - `message` (containing the raw Squid log line)
5.  Navigate to **Analytics > Dashboards** and search for "Squid" to view the pre-built **[Logs Squid] Overview** dashboard, which should now be populating with charts and maps.

# Troubleshooting

## Common Configuration Issues

- **Port Binding Conflicts**: If the Elastic Agent fails to start the listener, check if another service is already using the configured UDP/TCP port by running `sudo netstat -tulpn | grep <port>`.
- **Squid Syntax Errors**: If Squid fails to restart, check the configuration syntax using `squid -k parse`. Errors in the `logformat` or `access_log` directives will be highlighted in the output.
- **Firewall Blockage**: Ensure that the host firewall (e.g., `ufw` or `firewalld`) on both the Squid server and the Elastic Agent server allows traffic on the specified logging port.
- **SELinux/AppArmor Restrictions**: On some secured systems, Squid may be prevented from making outbound network connections for logging. Check `/var/log/audit/audit.log` for denial messages and adjust policies if necessary.

## Ingestion Errors

- **Parsing Failures**: If logs appear in Kibana with a `tags: ["_grokparsefailure"]`, the Squid log format in `squid.conf` may not match what the integration expects. Ensure you are using the default `squid` format or have properly mapped your custom format.
- **Missing Fields**: If certain ECS fields are empty, verify that the `logformat` directive in Squid includes the necessary macros (like `%rm` for request method or `%ru` for URL).
- **Timestamp Mismatches**: Ensure the time synchronization (NTP) is consistent between the Squid server and the Elastic Stack to avoid issues with logs appearing in the "future" or "past" in Discover.
- **Error Identification**: Check the `error.message` field in Kibana for specific details on why a particular log line failed to be processed by the ingest pipeline.

## Vendor Resources

- [Squid configuration directive access_log](https://www.squid-cache.org/Doc/config/access_log/)
- [Feature: Log Modules for Squid | Squid Web Cache wiki](https://wiki.squid-cache.org/Features/LogModules)
- Refer to the official vendor website for additional resources.

# Documentation sites

- [Elastic Squid Integration Reference](https://www.elastic.co/docs/reference/integrations/squid)
- Refer to the official vendor website for additional documentation.
